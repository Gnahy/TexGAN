{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tex_GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY6rHxQIYdfx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a16e0bf-9f40-472f-94a4-8c6186916846"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiqCVB03Yhyy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ab28792-b2e4-4107-aacd-c9f8e733c0eb"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYUr9u8nYrZr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000b0ca3-c3d6-441b-c277-edc76ec0e6fa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgUo77_DZMVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79b47367-37b4-40b7-b67c-fa2d91c71dbe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GX5kIl3JucxJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "749d2026-156b-4209-ddb3-dc48ce695432"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 5501558397857718715, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 18249337695816589617\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 3173658547479490101\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14912199066\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 2514787871059215254\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CK-gp9BZVo1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6b82586-42e6-448a-a3fb-8831b5285c75"
      },
      "source": [
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
        "from keras.layers import Flatten, BatchNormalization, Dense, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43VsSc0Zac7"
      },
      "source": [
        "def load_dataset(dataset_path, batch_size, image_shape):\n",
        "    dataset_generator = ImageDataGenerator()\n",
        "    dataset_generator = dataset_generator.flow_from_directory(\n",
        "        dataset_path, target_size=(image_shape[0], image_shape[1]),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None)\n",
        "\n",
        "    return dataset_generator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImFqDZ0XZmJw"
      },
      "source": [
        "def construct_discriminator(image_shape):\n",
        "\n",
        "    discriminator = Sequential()\n",
        "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
        "    discriminator.add(Conv2D(filters=64, kernel_size=(5, 5),\n",
        "                             strides=(2, 2), padding='same',\n",
        "                             data_format='channels_last',\n",
        "                             kernel_initializer=init,\n",
        "                             input_shape=(image_shape)))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    discriminator.add(Conv2D(filters=128, kernel_size=(5, 5),\n",
        "                             strides=(2, 2), padding='same',\n",
        "                             data_format='channels_last',\n",
        "                             kernel_initializer=init))\n",
        "    discriminator.add(BatchNormalization(momentum=0.5))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    discriminator.add(Conv2D(filters=256, kernel_size=(5, 5),\n",
        "                             strides=(2, 2), padding='same',\n",
        "                             data_format='channels_last',\n",
        "                             kernel_initializer=init))\n",
        "    discriminator.add(BatchNormalization(momentum=0.5))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    discriminator.add(Conv2D(filters=512, kernel_size=(5, 5),\n",
        "                             strides=(2, 2), padding='same',\n",
        "                             data_format='channels_last',\n",
        "                             kernel_initializer=init))\n",
        "    discriminator.add(BatchNormalization(momentum=0.5))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "\n",
        "    discriminator.add(Flatten())\n",
        "    discriminator.add(Dense(1))\n",
        "    discriminator.add(Activation('sigmoid'))\n",
        "\n",
        "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                          optimizer=optimizer,\n",
        "                          metrics=None)\n",
        "\n",
        "    return discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pDX0WzRZqDB"
      },
      "source": [
        "def construct_generator():\n",
        "\n",
        "    generator = Sequential()\n",
        "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
        "    generator.add(Dense(units=4 * 4 * 512,\n",
        "                        kernel_initializer='glorot_uniform',\n",
        "                        input_shape=(1, 1, 100)))\n",
        "    generator.add(Reshape(target_shape=(4, 4, 512)))\n",
        "    generator.add(BatchNormalization(momentum=0.5))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Conv2DTranspose(filters=256, kernel_size=(5, 5),\n",
        "                                  strides=(2, 2), padding='same',\n",
        "                                  data_format='channels_last',\n",
        "                                  kernel_initializer=init))\n",
        "    generator.add(BatchNormalization(momentum=0.5))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Conv2DTranspose(filters=128, kernel_size=(5, 5),\n",
        "                                  strides=(2, 2), padding='same',\n",
        "                                  data_format='channels_last',\n",
        "                                  kernel_initializer=init))\n",
        "    generator.add(BatchNormalization(momentum=0.5))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Conv2DTranspose(filters=64, kernel_size=(5, 5),\n",
        "                                  strides=(2, 2), padding='same',\n",
        "                                  data_format='channels_last',\n",
        "                                  kernel_initializer=init))\n",
        "    generator.add(BatchNormalization(momentum=0.5))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        "\n",
        "    generator.add(Conv2DTranspose(filters=3, kernel_size=(5, 5),\n",
        "                                  strides=(2, 2), padding='same',\n",
        "                                  data_format='channels_last',\n",
        "                                  kernel_initializer=init))\n",
        "    generator.add(Activation('tanh'))\n",
        "\n",
        "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
        "    generator.compile(loss='binary_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=None)\n",
        "\n",
        "    return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1skb6NFZ82u"
      },
      "source": [
        "def save_generated_images(generated_images, epoch, batch_number):\n",
        "\n",
        "    plt.figure(figsize=(8, 8), num=2)\n",
        "    gs1 = gridspec.GridSpec(8, 8)\n",
        "    gs1.update(wspace=0, hspace=0)\n",
        "\n",
        "    for i in range(64):\n",
        "        ax1 = plt.subplot(gs1[i])\n",
        "        ax1.set_aspect('equal')\n",
        "        image = generated_images[i, :, :, :]\n",
        "        image += 1\n",
        "        image *= 127.5\n",
        "        fig = plt.imshow(image.astype(np.uint8))\n",
        "        plt.axis('off')\n",
        "        fig.axes.get_xaxis().set_visible(False)\n",
        "        fig.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_name = './drive/My Drive/generated/gen' + str(\n",
        "        epoch + 1) + '_batch' + str(batch_number + 1) + '.png'\n",
        "\n",
        "    plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
        "    plt.pause(0.0000000001)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbWLoIn8aGeG"
      },
      "source": [
        "def train_dcgan(batch_size, epochs, image_shape, dataset_path):\n",
        "    # Build the adversarial model that consists in the generator output\n",
        "    # connected to the discriminator\n",
        "    generator = construct_generator()\n",
        "    discriminator = construct_discriminator(image_shape)\n",
        "    generator.load_weights('./drive/My Drive/generated/generator_epoch346.hdf5')\n",
        "    discriminator.load_weights('./drive/My Drive/generated/discriminator_epoch346.hdf5')\n",
        "\n",
        "    gan = Sequential()\n",
        "    # Only false for the adversarial model\n",
        "    discriminator.trainable = False\n",
        "    gan.add(generator)\n",
        "    gan.add(discriminator)\n",
        "\n",
        "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
        "                metrics=None)\n",
        "\n",
        "    # Create a dataset Generator with help of keras\n",
        "    dataset_generator = load_dataset(dataset_path, batch_size, image_shape)\n",
        "\n",
        "    # 11788 is the total number of images on the bird dataset\n",
        "    number_of_batches = int(15911 / batch_size)\n",
        "\n",
        "    # Variables that will be used to plot the losses from the discriminator and\n",
        "    # the adversarial models\n",
        "    adversarial_loss = np.empty(shape=1)\n",
        "    discriminator_loss = np.empty(shape=1)\n",
        "    batches = np.empty(shape=1)\n",
        "\n",
        "    # Allo plot updates inside for loop\n",
        "    plt.ion()\n",
        "\n",
        "    current_batch = 0\n",
        "\n",
        "    # Let's train the DCGAN for n epochs\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(\"Epoch \" + str(epoch+1) + \"/\" + str(epochs) + \" :\")\n",
        "\n",
        "        for batch_number in range(number_of_batches):\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Get the current batch and normalize the images between -1 and 1\n",
        "            real_images = dataset_generator.next()\n",
        "            real_images /= 127.5\n",
        "            real_images -= 1\n",
        "\n",
        "            # The last batch is smaller than the other ones, so we need to\n",
        "            # take that into account\n",
        "            current_batch_size = real_images.shape[0]\n",
        "\n",
        "            # Generate noise\n",
        "            noise = np.random.normal(0, 1,\n",
        "                                     size=(current_batch_size,) + (1, 1, 100))\n",
        "\n",
        "            # Generate images\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            # Add some noise to the labels that will be\n",
        "            # fed to the discriminator\n",
        "            real_y = (np.ones(current_batch_size) -\n",
        "                      np.random.random_sample(current_batch_size) * 0.2)\n",
        "            fake_y = np.random.random_sample(current_batch_size) * 0.2\n",
        "\n",
        "            # Let's train the discriminator\n",
        "            discriminator.trainable = True\n",
        "\n",
        "            d_loss = discriminator.train_on_batch(real_images, real_y)\n",
        "            d_loss += discriminator.train_on_batch(generated_images, fake_y)\n",
        "\n",
        "            discriminator_loss = np.append(discriminator_loss, d_loss)\n",
        "\n",
        "            # Now it's time to train the generator\n",
        "            discriminator.trainable = False\n",
        "\n",
        "            noise = np.random.normal(0, 1,\n",
        "                                     size=(current_batch_size * 2,) +\n",
        "                                     (1, 1, 100))\n",
        "\n",
        "            # We try to mislead the discriminator by giving the opposite labels\n",
        "            fake_y = (np.ones(current_batch_size * 2) -\n",
        "                      np.random.random_sample(current_batch_size * 2) * 0.2)\n",
        "\n",
        "            g_loss = gan.train_on_batch(noise, fake_y)\n",
        "            adversarial_loss = np.append(adversarial_loss, g_loss)\n",
        "            batches = np.append(batches, current_batch)\n",
        "\n",
        "            # Each 50 batches show and save images\n",
        "            #if((batch_number + 1) % 10 == 0 and\n",
        "            #   current_batch_size == batch_size):\n",
        "            #    save_generated_images(generated_images, epoch, batch_number)\n",
        "\n",
        "            time_elapsed = time.time() - start_time\n",
        "\n",
        "            # Display and plot the results\n",
        "            print(\"     Batch \" + str(batch_number + 1) + \"/\" +\n",
        "                  str(number_of_batches) +\n",
        "                  \" generator loss | discriminator loss : \" +\n",
        "                  str(g_loss) + \" | \" + str(d_loss) + ' - batch took ' +\n",
        "                  str(time_elapsed) + ' s.')\n",
        "\n",
        "            current_batch += 1\n",
        "\n",
        "        # Save the model weights each 5 epochs\n",
        "        #if (epoch) % 10 == 0:\n",
        "        #    discriminator.trainable = True\n",
        "        #    generator.save('./drive/My Drive/generated/eth_epoch_gen' + str(epoch+1) + '.hdf5')\n",
        "        #    discriminator.save('./drive/My Drive/generated/eth_epoch_dis' + str(epoch+1) + '.hdf5')\n",
        "\n",
        "        # Each epoch update the loss graphs\n",
        "        plt.figure(1)\n",
        "        plt.plot(batches, adversarial_loss, color='green',\n",
        "                 label='Generator Loss')\n",
        "        plt.plot(batches, discriminator_loss, color='blue',\n",
        "                 label='Discriminator Loss')\n",
        "        plt.title(\"DCGAN Train\")\n",
        "        plt.xlabel(\"Batch Iteration\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        if epoch == 0:\n",
        "            plt.legend()\n",
        "        plt.pause(0.0000000001)\n",
        "        plt.show()\n",
        "        save_name1 = './drive/My Drive/loss/trainingLossPlot' + str(epoch+1)+ '.jpg'\n",
        "        plt.savefig(save_name1, bbox_inches='tight', pad_inches=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tRykOfQaJnB"
      },
      "source": [
        "def main():\n",
        "    dataset_path = './drive/My Drive/pattern'\n",
        "    batch_size = 64\n",
        "    image_shape = (64, 64, 3)\n",
        "    epochs = 500\n",
        "    train_dcgan(batch_size, epochs,\n",
        "                image_shape, dataset_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uy2z3WaaSob"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvsi4Ly_iImf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}